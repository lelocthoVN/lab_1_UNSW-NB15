{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection on UNSW-NB15 Dataset\n",
    "\n",
    "–≠—Ç–æ—Ç notebook –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π –≤ —Å–µ—Ç–µ–≤–æ–º —Ç—Ä–∞—Ñ–∏–∫–µ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ UNSW-NB15.\n",
    "\n",
    "## –ü–ª–∞–Ω —Ä–∞–±–æ—Ç—ã:\n",
    "1. **–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö**\n",
    "2. **Exploratory Data Analysis (EDA)**\n",
    "3. **Feature Engineering –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞**\n",
    "4. **–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π anomaly detection**\n",
    "5. **–û—Ü–µ–Ω–∫–∞ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤**\n",
    "6. **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "\n",
    "# ML –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏\n",
    "np.random.seed(42)\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"‚úì –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏\n",
    "with open('../config/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞:\")\n",
    "print(f\"- –ú–æ–¥–µ–ª–∏: {list(config['models'].keys())}\")\n",
    "print(f\"- –ì—Ä—É–ø–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {list(config['features'].keys())}\")\n",
    "print(f\"- –ü—É—Ç–∏: {list(config['paths'].keys())}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö UNSW-NB15\n",
    "\n",
    "try:\n",
    "    train_data = pd.read_csv('../data/UNSW_NB15_training-set.csv')\n",
    "    test_data = pd.read_csv('../data/UNSW_NB15_testing-set.csv')\n",
    "    print(f\"‚úì –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã:\")\n",
    "    print(f\"  - Train: {train_data.shape}\")\n",
    "    print(f\"  - Test: {test_data.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå –§–∞–π–ª—ã –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!\")\n",
    "    print(\"–°–∫–∞—á–∞–π—Ç–µ UNSW-NB15 dataset –∏ –ø–æ–º–µ—Å—Ç–∏—Ç–µ CSV —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫—É data/\")\n",
    "    print(\"–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–º–∞–Ω–¥—É CLI –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏\")\n",
    "\n",
    "    # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\n",
    "    print(\"\\nüîÑ –°–æ–∑–¥–∞–µ–º –¥–µ–º–æ-–¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è...\")\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "\n",
    "    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ, –ø–æ—Ö–æ–∂–∏–µ –Ω–∞ UNSW-NB15\n",
    "    demo_data = {\n",
    "        'dur': np.random.exponential(0.1, n_samples),\n",
    "        'sbytes': np.random.lognormal(8, 2, n_samples),\n",
    "        'dbytes': np.random.lognormal(7, 2, n_samples),\n",
    "        'sttl': np.random.choice([64, 128, 255], n_samples),\n",
    "        'dttl': np.random.choice([64, 128, 255], n_samples),\n",
    "        'spkts': np.random.poisson(50, n_samples),\n",
    "        'dpkts': np.random.poisson(45, n_samples),\n",
    "        'sload': np.random.normal(1000, 200, n_samples),\n",
    "        'dload': np.random.normal(800, 150, n_samples),\n",
    "        'label': np.random.choice([0, 1], n_samples, p=[0.8, 0.2])\n",
    "    }\n",
    "\n",
    "    train_data = pd.DataFrame(demo_data)\n",
    "    test_data = train_data.sample(frac=0.3).copy()\n",
    "\n",
    "    print(f\"‚úì –î–µ–º–æ-–¥–∞–Ω–Ω—ã–µ —Å–æ–∑–¥–∞–Ω—ã: {train_data.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# –û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
    "print(\"=== –û–ë–©–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –î–ê–¢–ê–°–ï–¢–ï ===\")\n",
    "print(f\"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å train: {train_data.shape}\")\n",
    "print(f\"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å test: {test_data.shape}\")\n",
    "print(f\"\\n–¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö:\")\n",
    "print(train_data.dtypes.value_counts())\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤\n",
    "missing_data = train_data.isnull().sum()\n",
    "if missing_data.sum() > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∏ –≤ –¥–∞–Ω–Ω—ã—Ö:\")\n",
    "    print(missing_data[missing_data > 0].sort_values(ascending=False))\n",
    "else:\n",
    "    print(\"\\n‚úì –ü—Ä–æ–ø—É—Å–∫–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç\")\n",
    "\n",
    "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –º–µ—Ç–∫–∞–º (–µ—Å–ª–∏ –µ—Å—Ç—å)\n",
    "if 'label' in train_data.columns:\n",
    "    print(f\"\\n=== –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ö–õ–ê–°–°–û–í ===\")\n",
    "    label_counts = train_data['label'].value_counts()\n",
    "    print(label_counts)\n",
    "    print(f\"–î–æ–ª—è –∞–Ω–æ–º–∞–ª–∏–π: {label_counts.get(1, 0) / len(train_data) * 100:.2f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –∫–ª—é—á–µ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏\n",
    "key_features = (config['features']['network_basic'][:4] +\n",
    "                config['features']['transport'][:4])\n",
    "\n",
    "# –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "existing_features = [f for f in key_features if f in train_data.columns]\n",
    "\n",
    "if existing_features:\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, feature in enumerate(existing_features):\n",
    "        if i < 8:  # –º–∞–∫—Å–∏–º—É–º 8 –≥—Ä–∞—Ñ–∏–∫–æ–≤\n",
    "            # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞\n",
    "            axes[i].hist(train_data[feature], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "            axes[i].set_title(f'–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ {feature}')\n",
    "            axes[i].set_xlabel(feature)\n",
    "            axes[i].set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')\n",
    "\n",
    "            # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ\n",
    "            if train_data[feature].max() / train_data[feature].min() > 1000:\n",
    "                axes[i].set_yscale('log')\n",
    "\n",
    "    # –°–∫—Ä—ã–≤–∞–µ–º –ª–∏—à–Ω–∏–µ subplot'—ã\n",
    "    for i in range(len(existing_features), 8):\n",
    "        axes[i].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤', y=1.02, fontsize=16)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è –ö–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –¥–∞–Ω–Ω—ã—Ö\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑\n",
    "numeric_cols = train_data.select_dtypes(include=[np.number]).columns\n",
    "# –ò—Å–∫–ª—é—á–∞–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞\n",
    "feature_cols = [col for col in numeric_cols if col not in ['label', 'id']]\n",
    "\n",
    "if len(feature_cols) > 1:\n",
    "    correlation_matrix = train_data[feature_cols].corr()\n",
    "\n",
    "    # –ù–∞—Ö–æ–¥–∏–º –≤—ã—Å–æ–∫–∏–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏\n",
    "    threshold = config['eda']['correlation_threshold']\n",
    "    high_corr = np.where((np.abs(correlation_matrix) > threshold) &\n",
    "                         (correlation_matrix != 1.0))\n",
    "    high_corr_pairs = [(correlation_matrix.index[x], correlation_matrix.columns[y],\n",
    "                        correlation_matrix.iloc[x, y])\n",
    "                       for x, y in zip(*high_corr) if x < y]\n",
    "\n",
    "    if high_corr_pairs:\n",
    "        print(f\"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω—ã –≤—ã—Å–æ–∫–∏–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ (>{threshold}):\")\n",
    "        for feat1, feat2, corr_val in high_corr_pairs[:10]:  # —Ç–æ–ø-10\n",
    "            print(f\"  {feat1} ‚Üî {feat2}: {corr_val:.3f}\")\n",
    "\n",
    "    # –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    sns.heatmap(correlation_matrix, mask=mask, annot=False, cmap='RdYlBu_r',\n",
    "                center=0, square=True, cbar_kws={'shrink': 0.8})\n",
    "    plt.title('–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π\n",
    "def prepare_features(data, config):\n",
    "    \"\"\"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏\"\"\"\n",
    "\n",
    "    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∫—Ä–æ–º–µ –∏—Å–∫–ª—é—á–µ–Ω–Ω—ã—Ö\n",
    "    all_features = []\n",
    "    for feature_group in ['network_basic', 'transport', 'statistical', 'contextual', 'additional']:\n",
    "        if feature_group in config['features']:\n",
    "            all_features.extend(config['features'][feature_group])\n",
    "\n",
    "    # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "    existing_features = [f for f in all_features if f in data.columns]\n",
    "\n",
    "    # –ò—Å–∫–ª—é—á–∞–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –∏ ID\n",
    "    exclude_features = config['features'].get('exclude', ['id', 'label', 'attack_cat'])\n",
    "    feature_columns = [f for f in existing_features if f not in exclude_features]\n",
    "\n",
    "    print(f\"–í—ã–±—Ä–∞–Ω–æ {len(feature_columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è:\")\n",
    "    print(feature_columns[:10], '...' if len(feature_columns) > 10 else '')\n",
    "\n",
    "    return feature_columns\n",
    "\n",
    "\n",
    "# –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "feature_columns = prepare_features(train_data, config)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "X_train = train_data[feature_columns].copy()\n",
    "X_test = test_data[feature_columns].copy()\n",
    "\n",
    "# –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤ (–∑–∞–ø–æ–ª–Ω—è–µ–º –º–µ–¥–∏–∞–Ω–æ–π)\n",
    "for col in X_train.columns:\n",
    "    if X_train[col].dtype in ['int64', 'float64']:\n",
    "        median_val = X_train[col].median()\n",
    "        X_train[col].fillna(median_val, inplace=True)\n",
    "        X_test[col].fillna(median_val, inplace=True)\n",
    "\n",
    "print(f\"\\n‚úì –ú–∞—Ç—Ä–∏—Ü—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã:\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  X_test: {X_test.shape}\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π\n",
    "if 'label' in train_data.columns:\n",
    "    y_train = train_data['label']\n",
    "    y_test = test_data['label'] if 'label' in test_data.columns else None\n",
    "    print(f\"  y_train: {y_train.shape}\")\n",
    "    if y_test is not None:\n",
    "        print(f\"  y_test: {y_test.shape}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ - –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è unsupervised –æ–±—É—á–µ–Ω–∏–µ\")\n",
    "    y_train = y_test = None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π\n",
    "models = {}\n",
    "pipelines = {}\n",
    "\n",
    "# 1. Isolation Forest\n",
    "iso_params = config['models']['isolation_forest']\n",
    "iso_forest = IsolationForest(**iso_params)\n",
    "iso_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', iso_forest)\n",
    "])\n",
    "\n",
    "print(\"üå≤ –û–±—É—á–µ–Ω–∏–µ Isolation Forest...\")\n",
    "iso_pipeline.fit(X_train)\n",
    "pipelines['Isolation Forest'] = iso_pipeline\n",
    "\n",
    "# 2. Local Outlier Factor\n",
    "lof_params = config['models']['lof']\n",
    "lof = LocalOutlierFactor(**lof_params)\n",
    "lof_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', lof)\n",
    "])\n",
    "\n",
    "print(\"üéØ –û–±—É—á–µ–Ω–∏–µ Local Outlier Factor...\")\n",
    "lof_pipeline.fit(X_train)\n",
    "pipelines['Local Outlier Factor'] = lof_pipeline\n",
    "\n",
    "# 3. One-Class SVM\n",
    "svm_params = config['models']['one_class_svm']\n",
    "oc_svm = OneClassSVM(**svm_params)\n",
    "svm_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', oc_svm)\n",
    "])\n",
    "\n",
    "print(\"‚öôÔ∏è –û–±—É—á–µ–Ω–∏–µ One-Class SVM...\")\n",
    "svm_pipeline.fit(X_train)\n",
    "pipelines['One-Class SVM'] = svm_pipeline\n",
    "\n",
    "print(\"\\n‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!\")\n",
    "print(f\"–û–±—É—á–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {list(pipelines.keys())}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. –û—Ü–µ–Ω–∫–∞ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –æ—Ç –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π\n",
    "predictions = {}\n",
    "anomaly_scores = {}\n",
    "\n",
    "for name, pipeline in pipelines.items():\n",
    "    print(f\"üìä –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –æ—Ç {name}...\")\n",
    "\n",
    "    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (-1 –¥–ª—è –∞–Ω–æ–º–∞–ª–∏–π, 1 –¥–ª—è –Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö)\n",
    "    pred = pipeline.predict(X_test)\n",
    "    predictions[name] = (pred == 1).astype(int)  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ 0/1\n",
    "\n",
    "    # Anomaly scores\n",
    "    model = pipeline.named_steps['model']\n",
    "    if hasattr(model, 'decision_function'):\n",
    "        scores = pipeline.decision_function(X_test)\n",
    "        anomaly_scores[name] = -scores  # –ò–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏\n",
    "    elif hasattr(model, 'score_samples'):\n",
    "        scores = model.score_samples(pipeline.named_steps['scaler'].transform(X_test))\n",
    "        anomaly_scores[name] = -scores\n",
    "    else:\n",
    "        anomaly_scores[name] = np.zeros(len(pred))\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏\n",
    "results_df = pd.DataFrame(predictions)\n",
    "scores_df = pd.DataFrame(anomaly_scores)\n",
    "\n",
    "print(f\"\\n‚úÖ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ–ª—É—á–µ–Ω—ã –¥–ª—è {len(pipelines)} –º–æ–¥–µ–ª–µ–π\")\n",
    "print(f\"–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞: {len(X_test)}\")\n",
    "\n",
    "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    "print(\"\\n=== –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ê–ù–û–ú–ê–õ–ò–ô ====\")\n",
    "for name in pipelines.keys():\n",
    "    n_anomalies = predictions[name].sum()\n",
    "    anomaly_rate = n_anomalies / len(predictions[name]) * 100\n",
    "    print(f\"{name:20s}: {n_anomalies:4d} –∞–Ω–æ–º–∞–ª–∏–π ({anomaly_rate:5.2f}%)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π (–µ—Å–ª–∏ –µ—Å—Ç—å –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏)\n",
    "if y_test is not None:\n",
    "    print(\"=== –û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê –ú–û–î–ï–õ–ï–ô ===\")\n",
    "\n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for name in pipelines.keys():\n",
    "        y_pred = predictions[name]\n",
    "        y_score = anomaly_scores[name]\n",
    "\n",
    "        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏\n",
    "        precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "\n",
    "        # ROC-AUC (–µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ)\n",
    "        try:\n",
    "            if len(np.unique(y_test)) > 1 and len(np.unique(y_score)) > 1:\n",
    "                roc_auc = roc_auc_score(y_test, y_score)\n",
    "            else:\n",
    "                roc_auc = np.nan\n",
    "        except:\n",
    "            roc_auc = np.nan\n",
    "\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-Score': f1,\n",
    "            'ROC-AUC': roc_auc\n",
    "        })\n",
    "\n",
    "    # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "    results_table = pd.DataFrame(results)\n",
    "    results_table = results_table.round(4)\n",
    "    print(results_table.to_string(index=False))\n",
    "\n",
    "    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # Precision, Recall, F1\n",
    "    metrics = ['Precision', 'Recall', 'F1-Score']\n",
    "    for i, metric in enumerate(metrics):\n",
    "        axes[i].bar(results_table['Model'], results_table[metric], alpha=0.7)\n",
    "        axes[i].set_title(f'{metric} –ø–æ –º–æ–¥–µ–ª—è–º')\n",
    "        axes[i].set_ylabel(metric)\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è –ò—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º supervised –æ—Ü–µ–Ω–∫—É\")\n",
    "    print(\"–î–ª—è –ø–æ–ª–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ —Å –º–µ—Ç–∫–∞–º–∏ –∫–ª–∞—Å—Å–æ–≤\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Å–µ–Ω—Å—É—Å–∞ –º–æ–¥–µ–ª–µ–π\n",
    "# –°–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π —Å—á–∏—Ç–∞—é—Ç –∫–∞–∂–¥—É—é —Ç–æ—á–∫—É –∞–Ω–æ–º–∞–ª–∏–µ–π?\n",
    "consensus_scores = results_df.sum(axis=1)\n",
    "\n",
    "print(\"=== –ê–ù–ê–õ–ò–ó –ö–û–ù–°–ï–ù–°–£–°–ê –ú–û–î–ï–õ–ï–ô ===\")\n",
    "consensus_counts = consensus_scores.value_counts().sort_index()\n",
    "print(\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π, —Å—á–∏—Ç–∞—é—â–∏—Ö —Ç–æ—á–∫—É –∞–Ω–æ–º–∞–ª–∏–µ–π:\")\n",
    "for score, count in consensus_counts.items():\n",
    "    percentage = count / len(consensus_scores) * 100\n",
    "    print(f\"  {score} –º–æ–¥–µ–ª–µ–π: {count:4d} —Ç–æ—á–µ–∫ ({percentage:5.2f}%)\")\n",
    "\n",
    "# –¢–æ—á–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –≤—Å–µ –º–æ–¥–µ–ª–∏ —Å—á–∏—Ç–∞—é—Ç –∞–Ω–æ–º–∞–ª—å–Ω—ã–º–∏\n",
    "unanimous_anomalies = consensus_scores == len(pipelines)\n",
    "n_unanimous = unanimous_anomalies.sum()\n",
    "print(f\"\\nüéØ –ï–¥–∏–Ω–æ–≥–ª–∞—Å–Ω–æ –ø—Ä–∏–∑–Ω–∞–Ω–Ω—ã—Ö –∞–Ω–æ–º–∞–ª–∏–π: {n_unanimous} ({n_unanimous / len(consensus_scores) * 100:.2f}%)\")\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Å–µ–Ω—Å—É—Å–∞\n",
    "plt.figure(figsize=(10, 6))\n",
    "consensus_counts.plot(kind='bar', alpha=0.7, color='lightcoral')\n",
    "plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–Ω—Å–µ–Ω—Å—É—Å–∞ –º–æ–¥–µ–ª–µ–π')\n",
    "plt.xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π, —Å—á–∏—Ç–∞—é—â–∏—Ö —Ç–æ—á–∫—É –∞–Ω–æ–º–∞–ª–∏–µ–π')\n",
    "plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ—á–µ–∫')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# PCA –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π\n",
    "print(\"üîç –°–æ–∑–¥–∞–Ω–∏–µ PCA –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π...\")\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω—è–µ–º PCA –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_test_scaled = StandardScaler().fit_transform(X_test)\n",
    "X_pca = pca.fit_transform(X_test_scaled)\n",
    "\n",
    "print(f\"–û–±—ä—è—Å–Ω–µ–Ω–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è PCA: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# –û–±—â–∏–π plot —Å –∫–æ–Ω—Å–µ–Ω—Å—É—Å–æ–º\n",
    "scatter = axes[0].scatter(X_pca[:, 0], X_pca[:, 1],\n",
    "                          c=consensus_scores, cmap='Reds',\n",
    "                          alpha=0.6, s=50)\n",
    "axes[0].set_title('–ö–æ–Ω—Å–µ–Ω—Å—É—Å –º–æ–¥–µ–ª–µ–π (—Ü–≤–µ—Ç = –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π)')\n",
    "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.3f})')\n",
    "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.3f})')\n",
    "plt.colorbar(scatter, ax=axes[0])\n",
    "\n",
    "# –û—Ç–¥–µ–ª—å–Ω—ã–µ plots –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏\n",
    "colors = ['normal', 'anomaly']\n",
    "color_map = {0: 'lightblue', 1: 'red'}\n",
    "\n",
    "for i, (name, pred) in enumerate(predictions.items()):\n",
    "    if i < 3:  # –º–∞–∫—Å–∏–º—É–º 3 –º–æ–¥–µ–ª–∏\n",
    "        ax = axes[i + 1]\n",
    "\n",
    "        # –ù–æ—Ä–º–∞–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏\n",
    "        normal_mask = pred == 0\n",
    "        ax.scatter(X_pca[normal_mask, 0], X_pca[normal_mask, 1],\n",
    "                   c='lightblue', alpha=0.6, s=30, label='Normal')\n",
    "\n",
    "        # –ê–Ω–æ–º–∞–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏\n",
    "        anomaly_mask = pred == 1\n",
    "        ax.scatter(X_pca[anomaly_mask, 0], X_pca[anomaly_mask, 1],\n",
    "                   c='red', alpha=0.8, s=50, label='Anomaly', marker='^')\n",
    "\n",
    "        ax.set_title(f'{name}')\n",
    "        ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.3f})')\n",
    "        ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.3f})')\n",
    "        ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# –°–∫—Ä—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π subplot –µ—Å–ª–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è\n",
    "if len(pipelines) < 3:\n",
    "    axes[-1].set_visible(False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# –ê–Ω–∞–ª–∏–∑ —Ç–æ–ø-–∞–Ω–æ–º–∞–ª–∏–π\n",
    "print(\"üî• –ê–Ω–∞–ª–∏–∑ –Ω–∞–∏–±–æ–ª–µ–µ –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö —Ç–æ—á–µ–∫\")\n",
    "\n",
    "# –í—ã–±–µ—Ä–µ–º –º–æ–¥–µ–ª—å —Å –ª—É—á—à–∏–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ (–∏–ª–∏ Isolation Forest –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)\n",
    "best_model_name = 'Isolation Forest'  # –ú–æ–∂–Ω–æ –ø–æ–º–µ–Ω—è—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ü–µ–Ω–∫–∏\n",
    "best_scores = anomaly_scores[best_model_name]\n",
    "best_predictions = predictions[best_model_name]\n",
    "\n",
    "# –ù–∞–π–¥–µ–º —Ç–æ–ø-10 —Å–∞–º—ã—Ö –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö —Ç–æ—á–µ–∫\n",
    "top_anomalies_idx = np.argsort(best_scores)[-10:]\n",
    "print(f\"\\n–¢–æ–ø-10 –∞–Ω–æ–º–∞–ª–∏–π –ø–æ –º–æ–¥–µ–ª–∏ {best_model_name}:\")\n",
    "\n",
    "for i, idx in enumerate(reversed(top_anomalies_idx)):\n",
    "    score = best_scores[idx]\n",
    "    consensus = consensus_scores.iloc[idx]\n",
    "    print(f\"{i + 1:2d}. –ò–Ω–¥–µ–∫—Å {idx:4d}: score={score:.4f}, –∫–æ–Ω—Å–µ–Ω—Å—É—Å={consensus}/{len(pipelines)}\")\n",
    "\n",
    "# –ü–æ–∫–∞–∑–∞—Ç—å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Ç–æ–ø-–∞–Ω–æ–º–∞–ª–∏–π\n",
    "top_anomalies_data = X_test.iloc[top_anomalies_idx]\n",
    "print(f\"\\n–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Ç–æ–ø-–∞–Ω–æ–º–∞–ª–∏–π:\")\n",
    "print(top_anomalies_data.describe())\n",
    "\n",
    "# –°—Ä–∞–≤–Ω–∏–º —Å –Ω–æ—Ä–º–∞–ª—å–Ω—ã–º–∏ —Ç–æ—á–∫–∞–º–∏\n",
    "normal_points_idx = np.where(best_predictions == 0)[0][:100]  # 100 —Å–ª—É—á–∞–π–Ω—ã—Ö –Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö\n",
    "normal_data = X_test.iloc[normal_points_idx]\n",
    "\n",
    "print(f\"\\n–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π (–ê–Ω–æ–º–∞–ª–∏–∏ vs –ù–æ—Ä–º–∞–ª—å–Ω—ã–µ):\")\n",
    "comparison = pd.DataFrame({\n",
    "    'Anomalies_mean': top_anomalies_data.mean(),\n",
    "    'Normal_mean': normal_data.mean(),\n",
    "})\n",
    "comparison['Difference'] = comparison['Anomalies_mean'] - comparison['Normal_mean']\n",
    "comparison['Ratio'] = comparison['Anomalies_mean'] / comparison['Normal_mean']\n",
    "\n",
    "# –ü–æ–∫–∞–∂–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º–∏ —Ä–∞–∑–ª–∏—á–∏—è–º–∏\n",
    "comparison_sorted = comparison.reindex(comparison['Difference'].abs().sort_values(ascending=False).index)\n",
    "print(comparison_sorted.head(10).round(3))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ –≤—ã–≤–æ–¥—ã"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è\n",
    "artifacts_dir = Path('../artifacts')\n",
    "models_dir = artifacts_dir / 'models'\n",
    "reports_dir = artifacts_dir / 'reports'\n",
    "\n",
    "artifacts_dir.mkdir(exist_ok=True)\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "reports_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π\n",
    "print(\"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π...\")\n",
    "for name, pipeline in pipelines.items():\n",
    "    model_filename = name.lower().replace(' ', '_').replace('-', '_') + '_model.joblib'\n",
    "    joblib.dump(pipeline, models_dir / model_filename)\n",
    "    print(f\"‚úì {name} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ {model_filename}\")\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    "results_with_data = X_test.copy()\n",
    "for name in pipelines.keys():\n",
    "    results_with_data[f'{name}_prediction'] = predictions[name]\n",
    "    results_with_data[f'{name}_score'] = anomaly_scores[name]\n",
    "\n",
    "results_with_data['consensus_score'] = consensus_scores\n",
    "results_with_data.to_csv(reports_dir / 'predictions_detailed.csv', index=False)\n",
    "print(f\"‚úì –î–µ—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ predictions_detailed.csv\")\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)\n",
    "if y_test is not None:\n",
    "    results_table.to_csv(reports_dir / 'model_comparison.csv', index=False)\n",
    "    print(f\"‚úì –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ model_comparison.csv\")\n",
    "\n",
    "print(f\"\\nüéâ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω! –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {artifacts_dir}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –í—ã–≤–æ–¥—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n",
    "\n",
    "### –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:\n",
    "...\n",
    "\n",
    "### –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:\n",
    "...\n",
    "\n",
    "### –î–ª—è production:\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
